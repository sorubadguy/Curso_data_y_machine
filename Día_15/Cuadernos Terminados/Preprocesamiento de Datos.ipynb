{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16e995be",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos\n",
    "\n",
    "En esta lección vamos a centrarnos en un aspecto fundamental de cualquier proyecto de **Machine Learning**, que es una parte muy importante de la *transformación de los datos*, y que se denomina **preprocesamiento de datos**. Más específicamente vamos a ver qué recursos nos brinda **Scikit-Learn** para hacer este procedimiento.\n",
    "\n",
    "Preparar correctamente nuestros datos es crucial para el rendimiento de los modelos que desarrollaremos más adelante. Datos mal preparados, o pobremente preparados, van a devolver malos resultados, así que a prestar atención a esto.\n",
    "\n",
    "**Scikit-Learn** es muy útil para *transformar* y para *normalizar* datos de manera efectiva, ya que nos brinda herramientas como `MinMaxScaler`, `StandardScaler` y `OneHotEncoder`, que, para relacionarlo con lo que vimos en la lección anterior, son **transformadores**. Ya hemos conocido algunos de ellos, y a otros los vamos a conocer ahora.\n",
    "\n",
    "Comencemos importando las librerías que vamos a necesitar para los ejemplos de esta lección, entre las que se encuentran estos 3 elementos. Todos ellos están almacenados en `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff473d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Llamamos a `preprocessing` solo una vez, para importar los 3 recursos de preprocesamiento\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd9f1d",
   "metadata": {},
   "source": [
    "### ¿Por Qué es Importante el Preprocesamiento?\n",
    "\n",
    "Como vimos en la lección anterior, al trabajar con datos en Machine Learning, primero tenemos que hacer *estimaciones*, luego *transformaciones*, y finalmente *predicciones*. Durante el momento de las transformaciones es esencial hacer el **proprocesamiento de los datos** ¿por qué?\n",
    "+ **Mejora del Rendimiento del Modelo**: Datos bien preparados pueden mejorar significativamente la precisión y la eficiencia de nuestros modelos.\n",
    "+ **Consistencia de los Datos**: Normalizar los datos asegura que todas las características de los datos contribuyan equitativamente para el aprendizaje del modelo, y así evitamos que ciertas características dominen por su escala.\n",
    "+ **Manejo de Datos Faltantes y Categóricos**: A menudo, los conjuntos de datos contienen valores faltantes o algunos campos que tienen etiquetas categóricas (y no numéricas) que necesitan ser tratadas antes del análisis.\n",
    "\n",
    "Ahora sí, veamos las herramientas que disponemos para el preprocesamiento.\n",
    "\n",
    "\n",
    "### MinMaxScaler\n",
    "\n",
    "`MinMaxScaler` es una herramienta que ya hemos usado, pero ahora la vamos a ver un poco más en detalle con algunos ejemplos, antes de pasar a las siguientes. Lo que hace `MinMaxScaler` es transformar las características, **cambiando la escala de sus valores** a un rango dado, que normalmente es entre `0` y `1`. \n",
    "\n",
    "Veamos un ejemplo concreto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0e6691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, -1,  2],\n",
       "       [ 2,  0,  0],\n",
       "       [ 0,  1, -1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[1, -1, 2],\n",
    "                [2, 0, 0],\n",
    "                [0, 1, -1]])\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50d68667",
   "metadata": {},
   "source": [
    "Y ahora vamos a usar a `MinMaxScaler` para pasar los datos originales a una escala entre `0` y `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9e73fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_escalada = scaler.fit_transform(data)\n",
    "data_escalada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "981db967",
   "metadata": {},
   "source": [
    "Antes de pasar a la siguiente herramienta, te quiero mostrar a `MinMaxScaler` con datos un poco más reales. Lo apliquemos al dataset `iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bbe7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1e35929",
   "metadata": {},
   "source": [
    "Creamos una instancia de `MinMaxScaler` (aunque en este cuaderno no es necesario, porque ya lo hice antes con el ejemplo anterior, pero lo repito solamente para que veas el proceso completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d494706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c5350c8",
   "metadata": {},
   "source": [
    "Ajustamos el `scaler` al dataset `iris`, y transformamos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7220f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_escalado = scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b23a24e",
   "metadata": {},
   "source": [
    "Y ahora mostramos algunos de los datos escalados para verificar la diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "661360a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos Originales\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "\n",
      "Datos Escalados\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Datos Originales\")\n",
    "print(X[:5])\n",
    "print(\"\\nDatos Escalados\")\n",
    "print(X_escalado[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "473110a6",
   "metadata": {},
   "source": [
    "### StandardScaler\n",
    "\n",
    "La siguente herramienta (o el siguiente transformador) que podemos usar para el preprocesamiento de nuestros datos, se llama **StandardScaler**, y se utiliza para escalar los datos de manera que los datos resultantes tengan una **media de `0`** y una **desviación estándar de `1`**. Esto es ideal para algoritmos que asumen que todas las características tienen una distribución normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ffc64ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler2 = StandardScaler()\n",
    "data_escalada2 = scaler2.fit_transform(data)\n",
    "data_escalada2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43dea421",
   "metadata": {},
   "source": [
    "Como puedes ver, al compararlo con los resultados de `MinMaxScaler` son evidentes las diferencias en lo que hace cada uno de ellos.\n",
    "\n",
    "`MinMaxScaler` hace su escala de acuerdo a los **valores mínimos y máximos** que le hayas pasado, mientras que `StandardScaler` se preocupa en escalar los datos cuidando **que la media sea `0` y que la desviación estándar sea `1`**, y eso nos devuelve resultados diferentes, pero escalados al fin y al cabo.\n",
    "\n",
    "Podemos comprobar la **desviación estándar** de estos datos escalados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecc160f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data_escalada2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c94258b1",
   "metadata": {},
   "source": [
    "### OneHotEncoder\n",
    "\n",
    "Y finalmente le toca el turno al tercer transformador que nos brinda Scikit-Learn para hacer preprocesamiento de datos, y se trata de **OneHotEncoder**.\n",
    "\n",
    "El **OneHotEncoder** solamente se usa para **variables categóricas** (no numéricas), y su objetivo es convertir a esas variables categóricas en una forma que pueda ser proporcionada a los algoritmos de Machine Learning.\n",
    "\n",
    "**OneHotEncoder** nos va a devolver una **columna binaria por cada categoría**, marcando con un `1` la presencia de una categoría.\n",
    "\n",
    "Supongamos que tenemos una característica categórica con tres categorías posibles: `rojo`, `verde`, y `azul`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "370f90a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['rojo'],\n",
       "       ['verde'],\n",
       "       ['azul'],\n",
       "       ['verde'],\n",
       "       ['verde'],\n",
       "       ['azul']], dtype='<U5')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorias = np.array([[\"rojo\"], [\"verde\"], [\"azul\"], [\"verde\"], [\"verde\"], [\"azul\"]])\n",
    "categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c567f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "data_codificada = encoder.fit_transform(categorias)\n",
    "data_codificada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a17f430a",
   "metadata": {},
   "source": [
    "*******\n",
    "\n",
    "Entonces en esta lección hemos profundizado en un aspecto de la transformación de datos, que se llama **preprocesamiento**, y que es fundamental para poder pasar nuestros datos a los algoritmos de machine learning, si es que queremos obtener resultados confiables.\n",
    "\n",
    "Hemos visto que **Scikit-Learn** nos brinda herramientas muy poderosas para transformar y normalizar esos datos, y con ejemplos prácticos, hemos visto cómo aplicar `MinMaxScaler`, `StandardScaler`, y `OneHotEncoder` para mejorar la calidad y la consistencia de nuestros datos.\n",
    "\n",
    "Espero que hayas encontrado útiles estos métodos y te sientas preparado para aplicarlos en tus proyectos de Data Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ed180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
