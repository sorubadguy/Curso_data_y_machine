{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc2ceea4",
   "metadata": {},
   "source": [
    "# Pipelines y Automatización de Flujos de Trabajo\n",
    "\n",
    "Bienvenido a esta nueva lección donde nos vamos a centrar en una de las herramientas más poderosas y eficientes que **Scikit-Learn** ofrece para la automatización de procesos en machine learning: los **pipelines**.\n",
    "\n",
    "Vamos a aprender a construir y a utilizar unos recursos llamados **pipelines** que sirven para **facilitar tanto la implementación como la evaluación** de modelos.\n",
    "\n",
    "\n",
    "### Pipelines en el mundo real\n",
    "\n",
    "Primero lo vamos a definir por fueras del mundo de la programación, ya que esto nos va a ayudar a comprenderlo un poco mejor.\n",
    "\n",
    "\"**Pipeline**\" en inglés significa **tuberías**. Ya sabes, una red de tubos que transportan algo como agua, gas o petróleo. Entonces un \"**Pipeline**\" es algo que canaliza un elemento fluido, para que se transporte desde un lugar a otro.\n",
    "\n",
    "Este concepto se utiliza metafóricamente en diversas industrias para describir *procesos* que tienen una *secuencia lineal y continua*, donde la salida de una fase se convierte directamente en la entrada de la siguiente hasta llegar al final del proceso. Por ejemplo, en la industria manufacturera, un pipeline podría referirse al flujo de producción que describe el recorrido que hace la materia prima desde que ingresa a la industria, hasta el producto terminado.\n",
    "\n",
    "\n",
    "### Pipelines en programación\n",
    "\n",
    "En programación, o más específicamente en **Scikit-Learn**, un *pipeline* es una *secuencia de transformaciones que terminan de un modelo final*.\n",
    "\n",
    "Lo que hace un pipeline es encapsular todas las secuencias de pasos que hemos visto, como el preprocesamiento y el modelado, para que puedan ser procesados como un todo. Esto nos asegura que todos los pasos se ejecuten en el orden correcto, de una forma que sea repetible, y que sean fáciles de ajustar y de validar.\n",
    "\n",
    "\n",
    "### Creación de un Pipeline Básico\n",
    "\n",
    "Para comprender cómo se implementa un pipeline en el código de Scikit-Learn, vamos a retomar un bloque de código que ya hemos usado. El código que ves en la siguiente celda, es el que usamos en la lección *\"Introducción a Scikit-Learn\"*, para identificar los *estimadores*, los *transformadores* y los *predictores*.\n",
    "\n",
    "Es un código muy básico que hace lo siguiente:\n",
    "+ toma la base de datos iris\n",
    "+ divide sus datos en conjuntos de entrenamiento y prueba\n",
    "+ escala los datos\n",
    "+ transforma los datos\n",
    "+ entrena al modelo\n",
    "+ hace predicciones\n",
    "+ evalua el desempeño de las predicciones\n",
    "\n",
    "Quiero usar este código que ya conoces, para que veas cuál es la diferencia entre trabajar con y sin pipelines.\n",
    "\n",
    "### Código original (sin pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fbc98c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las predicciones son: [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n",
      "La precisión del modelo es: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Cargamos el dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Dividimos el dataset en conjunto de entrenamiento y de prueba\n",
    "X_entrena, X_prueba, y_entrena, y_prueba = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Creamos una instancia del escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Estimador (StandardScaler): Aprendemos los parámetros de escalado con fit\n",
    "scaler.fit(X_entrena)\n",
    "\n",
    "# Transformador (StandardScaler): Aplicamos la transformación a los datos de entrenamiento y prueba\n",
    "X_entrena_escalado = scaler.transform(X_entrena)\n",
    "X_prueba_escalado = scaler.transform(X_prueba)\n",
    "\n",
    "# Creamos una instancia del modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# Estimador (LogisticRegression): Entrenamos el modelo con los datos escalados\n",
    "modelo.fit(X_entrena_escalado, y_entrena)\n",
    "\n",
    "# Predictor (LogisticRegression): Hacemos predicciones y evaluamos el modelo\n",
    "y_pred = modelo.predict(X_prueba_escalado)\n",
    "puntaje = modelo.score(X_prueba_escalado, y_prueba)\n",
    "print(f\"Las predicciones son: {y_pred}\")\n",
    "print(f\"La precisión del modelo es: {puntaje:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39112262",
   "metadata": {},
   "source": [
    "### Código modificado (con Pipelines)\n",
    "\n",
    "En la próxima celda he copiado ese mismo código, pero aquí he hecho las modificaciones necesarias para poner sus procesos dentro de un **pipeline**. Puedes ver ambas versiones, comparar las diferencias (que explicaré más abajo), y verificar que los resultados son exactamente iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ece8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las predicciones son: [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n",
      "La precisión del modelo es: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Cargamos el dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Dividimos el dataset en conjunto de entrenamiento y de prueba\n",
    "X_entrena, X_prueba, y_entrena, y_prueba = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('modelo', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_entrena, y_entrena)\n",
    "\n",
    "# Predictor (LogisticRegression): Hacemos predicciones y evaluamos el modelo\n",
    "y_pred = pipeline.predict(X_prueba)\n",
    "puntaje = pipeline.score(X_prueba, y_prueba)\n",
    "\n",
    "print(f\"Las predicciones son: {y_pred}\")\n",
    "print(f\"La precisión del modelo es: {puntaje:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b429d1ac",
   "metadata": {},
   "source": [
    "### Implementación del pipeline\n",
    "\n",
    "Ahora vamos a describir las diferencias entre ambos códigos, y a explicar por qué hemos hecho cada modificación.\n",
    "\n",
    "#### 1. Importación de `Pipeline`\n",
    "\n",
    "Lo primero que hicimos fue importar al recurso de Scikit-Learn que permite aplicar las pipelines\n",
    "```\n",
    "from sklearn.pipeline import Pipeline\n",
    "```\n",
    "\n",
    "##### 2. Declaración del pipeline\n",
    "\n",
    "Lo segundo es declarar el pipeline en sí mismo. ¿cómo? modificando la forma en que declaramos al **escalador** y al **modelo**. En el *código original* lo hicimos en lineas separadas e independientes entre sí, de la siguiente manera:\n",
    "```\n",
    "scaler = StandardScaler()\n",
    "modelo = LogisticRegression()\n",
    "```\n",
    "Pero en el código nuevo he eliminado esas líneas, y he declarado una variable `pipeline` que contiene la función `Pipeline()`, a la cual le pasamos como parámetro una *lista* compuesta de *tuplas* con los nombres y llamadas a los objetos en cuestión.\n",
    "```\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),   # Paso 1: Escalador\n",
    "    ('modelo', LogisticRegression())  # Paso 2: Modelo\n",
    "])\n",
    "```\n",
    "\n",
    "##### 3. Entrenamiento del pipeline\n",
    "\n",
    "La ventaja de que ahora tengamos a estos dos elementos dentro de nuestra tubería, o **pipeline**, es que ya no vamos a necesitar entrenarlos por separado. Ahora solamente tengo que entrenar a mi pipeline, y eso aplica para cada uno de sus elementos internos.\n",
    "\n",
    "Por lo tanto puedo eliminar las líneas de entrenamiento por separado:\n",
    "```\n",
    "scaler.fit(X_entrena)\n",
    "modelo.fit(X_entrena_escalado, y_entrena)\n",
    "```\n",
    "\n",
    "Y directamente procedo a entrenal al pipeline.\n",
    "```\n",
    "pipeline.fit(X_entrena, y_entrena)\n",
    "```\n",
    "\n",
    "##### 4. Proceso de transformaciones dentro del piepline\n",
    "\n",
    "La siguiente ventaja es que ahora no vamos a necesitar escribir el **proceso de transformación** que hacíamos con estas líneas dedicadas al escalamiento de los datos:\n",
    "```\n",
    "X_entrena_escalado = scaler.transform(X_entrena)\n",
    "X_prueba_escalado = scaler.transform(X_prueba)\n",
    "```\n",
    "\n",
    "Esas lineas pueden ser eliminadas a partir de ahora ¿Por qué? Porque cuando usas un pipeline en Scikit-Learn, la necesidad de transformar explícitamente los datos de entrenamiento y prueba se elimina.\n",
    "\n",
    "Esta es tal vez la parte más importante respecto de la magia de pipeline: el pipeline automáticamente gestiona de manera interna todas las transformaciones, y se asegura que se apliquen de manera adecuada, y en el orden correcto, durante el entrenamiento y la predicción. \n",
    "+ Durante el Entrenamiento (`fit()`): primero ajusta el `StandardScaler` a los datos de entrenamiento (`X_entrena`) para aprender los parámetros de escalado (es decir, la **media** y la **desviación estándar** de cada característica). Inmediatamente después, transforma esos mismos datos de entrenamiento usando los parámetros aprendidos. Después de escalar los datos de entrenamiento, el pipeline pasa estos datos transformados directamente al modelo `LogisticRegression`, que entonces se entrena con ellos.\n",
    "+ Durante la Predicción (`predict()` y `score()`) el pipeline automáticamente transforma los datos de prueba (`X_prueba`) utilizando el mismo `StandardScaler` que fue ajustado a los datos de entrenamiento. Esto nos garantiza que los datos de prueba se escalen exactamente de la misma manera que los datos de entrenamiento.\n",
    "\n",
    "\n",
    "##### 5. Predicción y evaluación del pipeline\n",
    "\n",
    "Esto nos lleva a la última modificación que voy a hacer aquí, que es reemplazar `modelo` por `pipeline` en las predicciones y en la evaluación, y reemplazar estas variables que ya no existen, por las variables originales.\n",
    "```\n",
    "y_pred = pipeline.predict(X_prueba)\n",
    "puntaje = pipeline.score(X_prueba, y_prueba)\n",
    "```\n",
    "\n",
    "Al ejecutarlo, como puedes ver, obtuve exactamente lo mismo que en el código sin pipelines, pero con un abordaje que no solo es más eficiente, sino que tiene otros beneficios clave:\n",
    "- **Consistencia:** Garantiza que todos los datos se procesen de la misma forma. No hay riesgo de olvidar transformar los datos de prueba o de aplicar una transformación de manera incorrecta.\n",
    "- **Simplicidad:** Simplifica el código y reduce la posibilidad de errores, especialmente en flujos de trabajo de machine learning más complejos.\n",
    "- **Reproducibilidad:** Mejora la reproducibilidad de los resultados, ya que los mismos pasos y transformaciones se aplican de forma uniforme cada vez que se ejecuta el código.\n",
    "\n",
    "\n",
    "### Pipelines aún más simplificados\n",
    "\n",
    "Pero eso no es todo, hay una forma aún más simple de usar pipelines, que es a través de un recurso muy parecido, pero que se llama `make_pipeline`.\n",
    "\n",
    "El siguiente código es una copia exacta del anterior (el que ya tiene `pipeline`), pero donde introduje una sutil modificación:\n",
    "\n",
    "1. En lugar de importar `Pipeline`, he importado `make_pipeline`.\n",
    "2. La variable `pipeline` ya no contiene a la función `Pipeline()`, sino a la función `make_pipeline`.\n",
    "3. La función `make_pipeline()` no necesita que declaremos explícitamente los nombres de sus componentes, por lo que en lugar de una *lista* de *tuplas*, simplemente le pasamos los componentes en sí mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48da79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las predicciones son: [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n",
      "La precisión del modelo es: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Cargamos el dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Dividimos el dataset en conjunto de entrenamiento y de prueba\n",
    "X_entrena, X_prueba, y_entrena, y_prueba = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression())\n",
    "\n",
    "pipeline.fit(X_entrena, y_entrena)\n",
    "\n",
    "# Predictor (LogisticRegression): Hacemos predicciones y evaluamos el modelo\n",
    "y_pred = pipeline.predict(X_prueba)\n",
    "puntaje = pipeline.score(X_prueba, y_prueba)\n",
    "\n",
    "print(f\"Las predicciones son: {y_pred}\")\n",
    "print(f\"La precisión del modelo es: {puntaje:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "245b3c86",
   "metadata": {},
   "source": [
    "Al ejecutarlo obtenemos los mismos resultados pero con un código aún más simplificado y robusto.\n",
    "\n",
    "En esta lección, hemos explorado cómo los **pipelines** de **Scikit-Learn** nos permiten simplificar y automatizar el flujo de trabajo en proyectos de **Machine Learning**, desde el preprocesamiento de datos hasta la evaluación del modelo.\n",
    "\n",
    "Al integrar estas herramientas en tu práctica, vas a poder hacer que tus proyectos sean más eficientes, más efectivos y más fáciles de manejar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788f4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
