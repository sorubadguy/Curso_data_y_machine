{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fc3ee77",
   "metadata": {},
   "source": [
    "# Selección y División de Datos\n",
    "\n",
    "Bienvenido a esta nueva lección en la que vamos a avanzar un paso más en la estructora del código de **Scikit-Learn**. En la lección anterior analizamos el preprocesamiento de los datos, y ahora vamos a explorar cómo **preparar nuestros datos** de manera efectiva para el momento del entrenamiento y de la evaluación de los modelos en **Machine Learning**.\n",
    "\n",
    "\n",
    "### ¿Qué es la preparación de los datos?\n",
    "\n",
    "Como seguramente recuerdas, en todos los ejercicios que hicimos durante los días de Machine Learning, una de las cosas que hacíamos con nuestros datos era **dividirlos en conjuntos de prueba y conjuntos de entrenamiento**. Bueno, ese es el proceso que vamos a analizar en detalle y a expandir en esta lección.\n",
    "\n",
    "Vamos a aprender dos procesos de preparación de datos:\n",
    "+ **División de Datos**: En conjuntos de entrenamiento y prueba utilizando `train_test_split()`\n",
    "+ **Selección de Características más Importantes**: Específicamente con `SelectKBest` y `SelectFromModel`, dos nuevos amigos que agregamos a nuestro equipo\n",
    "\n",
    "Comencemos importando todas las librerías que necesitaremos en esta lección. Observa que he incluido al modelo `RandomforestClassifier`, a los nuevos recursos que te he mencionado, y a un elemento llamado `chi2` que también neceitaremos para la selección de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f66234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed669524",
   "metadata": {},
   "source": [
    "Para esta práctica vamos a utilizar nuevamente el conjunto de datos `iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb968df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed51e366",
   "metadata": {},
   "source": [
    "## División de Datos con `train_test_split()`\n",
    "\n",
    "Para repasar un poco, te recuerdo que el motivo por el que queremos dividir a nuestros datos en conjuntos de **entrenamiento** y **prueba**, es para poder evaluar la capacidad de nuestros modelos de aprendizaje automático.\n",
    "\n",
    "Al tener un conjunto de entrenamiento por un lado, y un conjunto de prueba por el otro, podemos ver si las **relaciones que ha identificado** nuestro modelo durante la etapa de estimaciones, se comprueban como correctas al verificar si se cumplen con el conjunto de pruebas.\n",
    "\n",
    "Por eso es que **Scikit-Learn** nos ofrece una función perfecta para esto, que es `train_test_split()`.\n",
    "\n",
    "Comencemos con un ejemplo clásico de cómo usamos esta función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24c5c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_entrena, X_prueba, y_entrena, y_prueba = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0a0d2",
   "metadata": {},
   "source": [
    "Ahora vamos a imprimir algunos datos, para comprender mejor qué tenemos aquí. Te sugiero probar distintos valores para el parámetro `test_size` de modo que veas cómo se reparten los casos en los diferentes conjuntos de entrenamiento y de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfbca0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto total: 150\n",
      "Tamaño del conjunto entrenamiento: 120\n",
      "Tamaño del conjunto pruebas: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño del conjunto total:\", len(X))\n",
    "print(\"Tamaño del conjunto entrenamiento:\", len(X_entrena))\n",
    "print(\"Tamaño del conjunto pruebas:\", len(X_prueba))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "530c182c",
   "metadata": {},
   "source": [
    "Antes de avanzar a la siguiente técnica, te propongo revisar la ayuda de `train_test_split()` para conocer todos sus detalles. Presta especial atención a la **descripción** de esta función, a sus **parámetros**, y a lo que **devuelve**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4306bde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
      "    Split arrays or matrices into random train and test subsets.\n",
      "    \n",
      "    Quick utility that wraps input validation,\n",
      "    ``next(ShuffleSplit().split(X, y))``, and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data into a\n",
      "    one-liner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "        Read more in the :ref:`User Guide <stratification>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cadd5815",
   "metadata": {},
   "source": [
    "## Selección de Características\n",
    "\n",
    "Además de la división de nuestros conjuntos de datos, otra estrategia que podemos implementar, es la **selección de características**. Este es un proceso mediante el cual seleccionamos automáticamente aquellas características que son **más relevantes para predecir** la variable objetivo.\n",
    "\n",
    "Recuerda que cuando usamos modelos de Machine Learning, nuestra meta es **realizar predicciones**. Eso significa que vamos a querer identificar cómo podemos predecir una determinada variable (la variable objetivo) a partir de cálculos sofisticados con **las variables que ya conocemos**. Esto nos lleva a una pregunta obvia: sin tengo 10 características en mi dataset *¿cuáles son las más importantes para predecir el valor de la variable objetivo?*.\n",
    "\n",
    "Para eso es importante contar con herramientas que nos permitan hacer una buena selección de categorías, que ayuden a mejorar la eficiencia del modelo y a reducir el riesgo de sobreajuste. Aquí es donde entran los objetos `SelectKBest`, y `SelectFromModel`. Vamos a conocerlos.\n",
    "\n",
    "\n",
    "### SelectKBest\n",
    "\n",
    "`SelectKBest` selecciona una cantidad determinada de características más relevantes basadas en pruebas estadísticas univariadas. Esto significa que `SelectKBest` es útil cuando queremos mantener solo las características más importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee78707",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=3)\n",
    "X_nuevo = selector.fit_transform(X_entrena, y_entrena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d35673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 5 filas del conjunto de entrenamiento\n",
      "[[4.6 3.6 1.  0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.4 3.2 1.3 0.2]]\n",
      "\n",
      "Primeras 5 filas del conjunto de columnas seleccionadas\n",
      "[[4.6 1.  0.2]\n",
      " [5.7 1.5 0.4]\n",
      " [6.7 4.4 1.4]\n",
      " [4.8 1.6 0.2]\n",
      " [4.4 1.3 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeras 5 filas del conjunto de entrenamiento\")\n",
    "print(X_entrena[:5])\n",
    "print(\"\\nPrimeras 5 filas del conjunto de columnas seleccionadas\")\n",
    "print(X_nuevo[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9524e49e",
   "metadata": {},
   "source": [
    "Así, con este procedimiento hemos logrado crear un **nuevo array** que contiene solamente las **características más útiles** para realizar predicciones con nuestros datos.\n",
    "\n",
    "`SelectKBest` es muy útil cuando el número de características es relativamente grande y se necesita reducir la dimensionalidad para simplificar el modelo, para mejorar la eficiencia o para reducir el riesgo de sobreajuste. \n",
    "\n",
    "\n",
    "### SelectFromModel\n",
    "\n",
    "`SelectFromModel` es otra técnica que selecciona características basándose en la importancia de las características asignadas por un modelo predictivo.\n",
    "\n",
    "La principal diferencia que tiene con respecto a `SelectKBest`, es que `SelectFromModel` selecciona características basadas en la importancia de las características **asignadas por un modelo predictivo base**.\n",
    "\n",
    "Entonces, mientras que `SelectKBest` evalúa cada característica de manera **independiente del resto**, `SelectFromModel` puede considerar la **interacción entre las características**, ya que la selección se basa en un modelo de Machine Learning que puede captar esas interacciones.\n",
    "\n",
    "A continuación vamos a definir primero el **modelo** para usar en la selección de características (en este caso `RandomForestClassifier`), y luego generamos el **selector** propiamente dicho, que asume las interacciones captadas por ese modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7fa5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "selector2 = SelectFromModel(modelo)\n",
    "X_importante = selector2.fit_transform(X_entrena, y_entrena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee5e4d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 5 filas del conjunto de entrenamiento\n",
      "[[4.6 3.6 1.  0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.4 3.2 1.3 0.2]]\n",
      "\n",
      "Primeras 5 filas del conjunto de columnas seleccionadas\n",
      "[[1.  0.2]\n",
      " [1.5 0.4]\n",
      " [4.4 1.4]\n",
      " [1.6 0.2]\n",
      " [1.3 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeras 5 filas del conjunto de entrenamiento\")\n",
    "print(X_entrena[:5])\n",
    "print(\"\\nPrimeras 5 filas del conjunto de columnas seleccionadas\")\n",
    "print(X_importante[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd2df8",
   "metadata": {},
   "source": [
    "En este caso también ha devuelto **dos características** como las más importantes, y de hecho **son las mismas**. Esto se debe a que si ambos selectores trabajan con efectividad, van a llegar a los mismos resultados por caminos diferentes.\n",
    "\n",
    "Ten presente, eso sí, que si bien en `SelectKBest` hemos podido establecer explícitamente la cantidad de características importantes usando el parámetro `k`, eso es diferente en `SelectFromModel`, en el cual por defecto la cantidad de características que selecciona es igual al **promedio de las importancias de las características** calculadas por el modelo base.\n",
    "\n",
    "Si por alguna razón preferimos especificar otra cantidad de características diferentes, lo hacemos manipulando el parámetro `threshold`, que significa *umbral*. Pero ten en cuenta que según las características de los datos, modificar este parámetro puede generar advertencias.\n",
    "\n",
    "```selector2 = SelectFromModel(modelo, threshold=1)```\n",
    "\n",
    "\n",
    "## Cierre de la lección\n",
    "\n",
    "Con esto hemos cubierto lo más importante sobre **selección** y **división** de datos con **Scikit-Learn**, hemos aprendido cómo dividir correctamente nuestros datos en conjuntos de entrenamiento y prueba, y hemos explorado dos métodos muy poderosos para la selección de características.\n",
    "\n",
    "Estas técnicas son esenciales para preparar nuestros datos y para asegurar que los modelos de Machine Learning que desarrollamos sean robustos, eficientes y efectivos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8263a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
