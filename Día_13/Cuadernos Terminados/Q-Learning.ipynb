{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90e20e95",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "Y aunque no lo creas, aquí estamos, aprendiendo los algoritmos de **Aprendizaje por Reforzamiento**, y ya próximos a terminar de cubrir **Machine Learning** en estos 3 días tan provechosos.\n",
    "\n",
    "Vamos entonces con **Q-Learning**, el mprimero de ellos\n",
    "\n",
    "\n",
    "### ¿Y qué es Q-learning?\n",
    "\n",
    "Q-learning es un tipo de algoritmo de **Aprendizaje por Refuerzo** que permite a un *agente* aprender a tomar decisiones óptimas y alcanzar un *objetivo* en un *ambiente* determinado.\n",
    "\n",
    "El agente opera aprendiendo los valores más convenientes para cada paso que va a dar, y a esos pasos los implementamos en un par de datos que definen la **acción** y el **estado**.\n",
    "\n",
    "Entonces en **Q-learning** tenemos un agente que debe identificar qué pasos realizar, y la calidad de esos pasos se denomina con la letra *\"Q\"*, por eso se llama *\"Q-learning\"*.\n",
    "\n",
    "Los valores de *Q* (es decir, los valores de sus pasos, que se definen por los valores de **acción** y de **estado**) le ayudan al *agente* a decidir qué acción tomar en cada paso.\n",
    "\n",
    "Imagínate que tu eres un **agente** que está parado en una **esquina X** de una **ciudad** que no conoces, y que quieres llegar a un **parque** que se encuentra en otro punto de la ciudad.\n",
    "\n",
    "![ciudad](ciudad.png)\n",
    "\n",
    "Ahora imagina que puedes tomar todas las **decisiones posibles** para moverte por las calles de la ciudad, y que en función de la acción tomada y el estado en que te encuentras al tomar cada decisión te permite evaluar qué tan buena ha sido esa decisión, en función de tu objetivo que es llegar al parque. Sin duda que después de probar **todas las alternativas posibles**, habrás hecho un aprendizaje sobre el camino correcto y sobre cuáles son las mejores decisiones posibles para llegar al parque soñado.\n",
    "\n",
    "![ciudad](ciudad_pasos.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0e048b7",
   "metadata": {},
   "source": [
    "Para esto, lo que hace **Q-learning** detrás de la cortina, es actualizar los valores *Q* usando esta fórmula llamada la **ecuación de Bellman**.\n",
    "\n",
    "![](Bellman.png)\n",
    "\n",
    "No es necesario que la aprendas ni que la recuerdes. Solo te voy a describir muy superficialmente lo que hace para que luego, cuando escribamos código, entiendas para qué definimos ciertas variables.\n",
    "\n",
    "Lo que dice esta ecuación es que el valor *Q* para un *estado* dado y una *acción* dada se actualiza tomando el valor *Q* original para ese *estado* y esa *acción*, y sumándole una fracción (definida por la *tasa de aprendizaje*) constituida por la **diferencia** entre:\n",
    "+ La *recompensa* observada por tomar esa *acción* en ese *estado*, más el *valor máximo Q* estimado para el *siguiente estado* (que sería el *mejor valor Q* que podemos obtener desde el *siguiente estado*, considerando todas las posibles acciones futuras), multiplicado por el *factor de descuento* (que reduce el valor de las recompensas futuras),\n",
    "+ Y todo eso restando el *valor Q original* para el *estado* y *acción* actuales.\n",
    "\n",
    "\n",
    "### Basta de teoría\n",
    "\n",
    "¿Te animas a implementarlo con un caso concreto? Seguro que sí: todos mis estudiantes son personas valientes. Especialmente los que han avanzado tanto en el curso.\n",
    "\n",
    "Imaginemos un **robot** que opera en una **cuadrícula** que representa el salón de una fábrica en la que debe moverse trasladando herramientas. El objetivo es que el robot aprenda a encontrar el **camino más corto** desde su posición inicial hasta el **destino**, evitando **obstáculos**.\n",
    "\n",
    "![](robot.png)\n",
    "\n",
    "Entonces el entorno que vamos a definir será:\n",
    "+ Una cuadrícula con dimensiones de 5x5.\n",
    "+ El punto de inicio en la esquina superior izquierda (0,0).\n",
    "+ El punto objetivo en la esquina inferior derecha (4,4).\n",
    "+ Y los obstáculos distribuidos en la cuadrícula.\n",
    "\n",
    "Vamos a establecer las **acciones**, que implicarán que el robot puede moverse en cuatro direcciones: *arriba*, *abajo*, *izquierda*, y *derecha*.\n",
    "\n",
    "Y vamos a definir **recompensas**:\n",
    "\t• Alcanzar el objetivo: **+100**\n",
    "\t• Colisionar con un obstáculo: **-100**\n",
    "\t• Cualquier otro movimiento: **-1** (para incentivar la eficiencia)\n",
    "\n",
    "Bueno, ha llegado el momento de implementarlo en Python, pero ten en cuenta que dada la complejidad de implementar un entorno de simulación desde cero, este ejemplo va a estar bastante simplificado para poder ilustrar el concepto en una lección de duración normal.\n",
    "\n",
    "Siempre podrás ampliar estas habilidades con la documentación adecuada, pero yo me voy a asegurar de que comprendas estos conceptos para que luego estés en condiciones de crecer desde esta base.\n",
    "\n",
    "Aunque parezca mentira, las únicas librerías que necesitaremos serán **NumPy** y **random**, para que nuestro agente pueda hacer cosas aleatorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ccd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39bce1ce",
   "metadata": {},
   "source": [
    "El primer paso va a ser definir el entorno con sus **condiciones iniciales**. Esto sería básicamente como dibujar el tablero de juego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71fb0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensiones = (5, 5)\n",
    "estado_inicial = (0, 0)\n",
    "estado_objetivo = (4, 4)\n",
    "obstaculos = [(1, 1), (1, 3), (2, 3), (3, 0)]\n",
    "acciones = [(-1, 0), (1, 0), (0, -1), (0, 1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c423a05",
   "metadata": {},
   "source": [
    "El siguiente paso es inicializar una nueva tabla, que se llama **tabla Q**, para la cual necesitamos definir dos variables auxiliares.\n",
    "+ La primera es una variables que calcule el **número total de estados posibles** en el entorno. Es decir, según las dimensiones de la cuadrícula, cuántas posiciones posibles hay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20afbf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_estados = dimensiones[0] * dimensiones[1]\n",
    "num_estados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37be75e2",
   "metadata": {},
   "source": [
    "+ La segunda variable auxiliar que necesitará nuestra **tabla Q**, es una que nos diga **cuántas acciones diferentes** puede hacer nuestro robot en cada movida. En este caso sabemos que son cuatro, pero lo vamos a calcular de esta manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80375d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_acciones = len(acciones)\n",
    "num_acciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f454175",
   "metadata": {},
   "source": [
    "Y finalmente definimos nuestra **tabla Q**, que va a ser una **matriz de dos dimensiones**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640a87c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((num_estados, num_acciones))\n",
    "Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a6c8db5",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestra **tabla Q**, vamos a crear una **función** cuyo objetivo es convertir la representación bidimensional del *estado actual* (que sería la posición del robot en la cuadrícula) a un *índice lineal único*\n",
    "\n",
    "¿Para qué hacemos esto? Para trabajar de manera más efectiva con la **tabla Q**, que es un array que contiene **25 filas** (cuyos índices van desde **0** hasta **24**), y de esa manera, podemos hacer que cada estado posible se pueda representar por un **número de índice** de la **tabla Q**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67cfbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estado_a_indice(estado):\n",
    "    return estado[0] * dimensiones[1] + estado[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dd4a04d",
   "metadata": {},
   "source": [
    "Para que lo entendamos mejor apliquemos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a8ad96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ejemplo = estado_a_indice((1, 0))\n",
    "ejemplo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59ed4103",
   "metadata": {},
   "source": [
    "Puedes intentar jugar con algunos ejemplos más para identificar c+omo se realiza esta transformación.\n",
    "\n",
    "El siguiente bloque de código va a definir los **parámetros clave** que se van a utilizar en el algoritmo de **Q-learning** dentro del contexto de nuestro ejemplo de navegación autónoma de un robot.\n",
    "\n",
    "Esta puede ser la parte más abstracta de nuestro código, por lo que puede ser la más difícil de capturar, pero te la voy a explicar lo más claramente posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e37a4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "episodios = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1c1cf",
   "metadata": {},
   "source": [
    "+ **`alpha = 0.1`** - Este es el factor de **tasa de aprendizaje**. Controla cuánto se actualiza el **valor Q** en cada paso del aprendizaje. Un valor de alpha más alto significa que la información más reciente tiene un peso mayor, permitiendo un aprendizaje más rápido pero potencialmente menos estable. Un valor más bajo hace que el aprendizaje sea más lento pero puede llevar a una estimación más estable de los **valores Q**.\n",
    "+ **`gamma = 0.99`** - Este es el **factor de descuento**. Lo que hace es determinar la importancia de las **recompensas futuras**. Si tenemos un valor de `gamma` cercano a `1` hace que las recompensas futuras sean casi tan importantes como las recompensas inmediatas, y de esa manera incentiva al agente a considerar consecuencias a largo plazo de sus acciones. Un valor más bajo haría que el agente valorase más las recompensas inmediatas.\n",
    "+ **`epsilon = 0.2`** - Este valor sirve para que el agente **no repita siempre las mismas decisiones** ¿cómo? Definiendo la probabilidad de que el agente tome una acción aleatoria en lugar de la mejor acción conocida hasta el momento según la *tabla Q*. Esto permite que el agente explore el entorno en lugar de explotar constantemente el conocimiento que ya dispone. Con este número queremos lograr un equilibrio entre exploración y explotación para asegurar que el agente siga aprendiendo eficazmente sobre el entorno.\n",
    "+ **`episodios = 1000`** - Aquí definimos el **número total de episodios** para el proceso de entrenamiento. Un **episodio** comienza con el agente en el estado inicial y termina cuando alcanza el objetivo o algún otro criterio de terminación. Mientras mayor sea el número de episodios, más oportunidades tendrá al agente de tener más experiencias de las cuales aprender, mejorando así potencialmente su política de acción.\n",
    "\n",
    "Con estos parámetros, que son fundamentales, hemos configurado cómo el *agente* va a aprender a navegar por el *entorno*. Puedes ajustar estos valores luego para evaluar cómo afectan los resultados, teniendo en cuenta que esto puede tener un impacto muy significativo en la eficacia y en la eficiencia del aprendizaje del agente.\n",
    "\n",
    "Ahora vamos a lo que a mí más me gusta, que es configurar funciones.\n",
    "\n",
    "La primera, servirá para que el *agente* pueda elegir una **acción** de las 4 que tiene disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "745a392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elegir_accion(estado):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(range(num_acciones))\n",
    "    else:\n",
    "        return np.argmax(Q[estado_a_indice(estado)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30c2e8f2",
   "metadata": {},
   "source": [
    "La segunda función servirá para **aplicar la acción elegida**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48834928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_accion(estado, accion_idx):\n",
    "    accion = acciones[accion_idx]\n",
    "    nuevo_estado = tuple(np.add(estado, accion) % dimensiones)\n",
    "    \n",
    "    if nuevo_estado in obstaculos or nuevo_estado == estado:\n",
    "        return estado, -100, False\n",
    "    if nuevo_estado == estado_objetivo:\n",
    "        return nuevo_estado, 100, True\n",
    "    return nuevo_estado, -1, False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdb7fec8",
   "metadata": {},
   "source": [
    "Con todo esto, ya estamos en condiciones de iniciar el **proceso de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9713f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodio in range(episodios):\n",
    "    estado = estado_inicial\n",
    "    terminado = False\n",
    "    \n",
    "    while not terminado:\n",
    "        idx_estado = estado_a_indice(estado)\n",
    "        accion_idx = elegir_accion(estado)\n",
    "        nuevo_estado, recompensa, terminado = aplicar_accion(estado, accion_idx)\n",
    "        idx_nuevo_estado = estado_a_indice(nuevo_estado)\n",
    "        \n",
    "        Q[idx_estado, accion_idx] = Q[idx_estado, accion_idx] + alpha * (recompensa + gamma * np.max(Q[idx_nuevo_estado]) - Q[idx_estado, accion_idx])\n",
    "        \n",
    "        estado = nuevo_estado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59cabe03",
   "metadata": {},
   "source": [
    "Muy bien, **nuestro modelo ha sido entrenado**, y ha definido una **política de acción** más conveniente para coada posición de nuestra cuadrícula.\n",
    "\n",
    "Aquí podríamos decir que el trabajo está terminado, porque nuestro *agente* ya le puede transmitir al robot qué acciones tomar en cada posición para llegar hasta el objetivo siguiendo el camino más eficiente y sin chocar con ningún obstáculo.\n",
    "\n",
    "Pero como nosotros además de lograr nuestros objetivos, nos gusta poder visualizar qué es lo que hemos hecho, vamos a diseñar una pequeña matriz que nos permita ver esto de un modo entendible.\n",
    "\n",
    "### Todo listo. Ahora a visualizar\n",
    "\n",
    "Crearemos una **matriz vacía** para la **política**, con las mismas dimensiones que el *entorno*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4806ff02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politica = np.zeros(dimensiones, dtype=int)\n",
    "politica"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "662ebcd8",
   "metadata": {},
   "source": [
    "Y finalmente vamos a llenar la matriz de política con la **mejor acción** para cada *estado*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57606a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política aprendida (0: arriba, 1: abajo, 2: izquierda, 3: derecha)\n",
      "[[0 2 0 3 0]\n",
      " [0 0 1 0 1]\n",
      " [1 2 0 0 0]\n",
      " [0 2 2 2 0]\n",
      " [2 1 0 3 0]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(dimensiones[0]):\n",
    "    for j in range(dimensiones[1]):\n",
    "        estado = (i, j)\n",
    "        idx_estado = estado_a_indice(estado)\n",
    "        mejor_accion = np.argmax(Q[idx_estado])\n",
    "        politica[i, j] = mejor_accion\n",
    "        \n",
    "print(\"Política aprendida (0: arriba, 1: abajo, 2: izquierda, 3: derecha)\")\n",
    "print(politica)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd02a463",
   "metadata": {},
   "source": [
    "Este resultado es una instantánea del conocimiento adquirido por el *agente*: cómo navegar el entorno de manera eficiente basado en las recompensas y penalizaciones experimentadas durante el entrenamiento.\n",
    "\n",
    "Y esto ha sido el **Q-learning**, el primero de los algoritmos de **Aprendizaje por Reforzamiento**. Te invito a aprender el segundo ahora mismo, en la siguiente lección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fe494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
