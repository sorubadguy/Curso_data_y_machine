{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a47b51c",
   "metadata": {},
   "source": [
    "# SARSA (Estado - Acción - Recompensa - Estado - Acción)\n",
    "\n",
    "Este tercer día está marchando genial, estamos cargados de energía y con muchas ganas de seguir aprendiendo, así que vamos a por el segundo algoritmo de **Aprendizaje por Reforzamiento**, que es uno llamado **SARSA**, y este curioso nombre proviene de las siglas en inglés de la expresión *\"Estado-Acción-Recompensa-Estado-Acción\"*.\n",
    "\n",
    "Comencemos a clarificar un poco este concepto. **SARSA** es un algoritmo que también se utiliza para aprender la política de un *agente*. Recuerda que cuando decimos la *mejor política*, estamos diciendo la *mejor acción a tomar* en cada *estado*.\n",
    "\n",
    "Esto es bastante similar a lo que hicimos con **Q-learning**, pero con la diferencia de mientras que Q-learning solo considera el *estado actual* del agente para tomar la siguiente decisión, **SARSA** toma en cuenta tanto el *estado actual* como la *acción actual* para predecir la próxima acción y su recompensa. Este enfoque se conoce como aprendizaje *\"on-policy\"*, donde el agente aprende el valor de la política que está siguiendo actualmente, incluyendo cómo decidirá sus acciones futuras.\n",
    "\n",
    "La actualización de los **valores Q** en **SARSA** sigue la fórmula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0be80",
   "metadata": {},
   "source": [
    "![](SARSA.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79354cc5",
   "metadata": {},
   "source": [
    "Esta ecuación podría leerse de esta manera:\n",
    "\n",
    "> \"El nuevo valor Q de un par estado-acción, es igual al valor Q anterior de (s, a), más el producto de multiplicar la tasa de aprendizaje alfa por la diferencia entre la recompensa observada r más el producto del factor de descuento gamma y el valor Q del siguiente estado y acción, Q de (s' , a'), menos el valor Q anterior de (s, a).\"\n",
    "\n",
    "¿Te vas a aprender eso de memoria? **Por supuesto que NO**. Yo tampoco. La idea, como siempre, es abrir el capot del auto durante un segundo para ver cómo es el motor, y ahora lo cerramos de inmediato, para sentarnos en el asiento del conductor y conducir nuestro coche usando el tablero de control que nos permitirá hacer funcionar ese motor de manera indirecta.\n",
    "\n",
    "A continuación, verás las librerías que usaremos en esta lección. Nada nuevo bajo el sol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a78d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee807f5a",
   "metadata": {},
   "source": [
    "### Recursos para el código: variables y funciones\n",
    "\n",
    "\n",
    "Vamos a hacer un ejercicio similar al de la lección anterior, donde tendremos una **cuadrícula de 4x4**, en la que esta vez no pondremos obstáculos.\n",
    "\n",
    "Entonces vamos a proceder a definir todas las variables que necesitaremos para este ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25766c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensiones = (4, 4)\n",
    "estado_inicial = (0, 0)\n",
    "estado_objetivo = (3, 3)\n",
    "acciones = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
    "acciones_simbolos = ['↑', '↓', '←', '→']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a49ec44",
   "metadata": {},
   "source": [
    "Lo que sigue, es inicializar la **tabla Q**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f73b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_estados = dimensiones[0] * dimensiones[1]\n",
    "num_estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cbae998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_acciones = len(acciones)\n",
    "num_acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780731ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((num_estados, num_acciones))\n",
    "Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3b55457",
   "metadata": {},
   "source": [
    "Nos falta definir los **parametros SARSA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd003c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "episodios = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6776a5ed",
   "metadata": {},
   "source": [
    "+ **`alpha = 0.1`**: Determina cuánto de la nueva información tiene que sobrescribir a la información antigua. Si ponemos una tasa de aprendizaje más alta permitimos que el agente ajuste sus estimaciones de **valor Q** más rápidamente como respuesta a la nueva información. Sin embargo, la desventaja de un valor demasiado alto es que puede hacer que el aprendizaje sea inestable. Entonces, un valor de 0.1 es un punto de partida común, que nos proporciona un equilibrio entre la estabilidad y la velocidad del aprendizaje.\n",
    "+ **`gamma = 0-99`**: Indica la importancia que van a tener las recompensas futuras. Mientras más cercano a `1` sea el valor, hace que el agente valore más las recompensas futuras y que actúe más estratégicamente en el largo plazo. Entonces un valor de 0.99 es bastante común en entornos en los que deseamos que el agente tenga en cuenta el futuro cercano de manera más significativa.\n",
    "+ **`epsilon = 0.2`**: Regula el equilibrio que tiene que haber entre la exploración de nuevas acciones y la explotación del conocimiento que ya ha sido adquirido. Poniendo un `epsilon` de `0.2`, tendremos un **20%** de probabilidad de que el agente elija una acción al azar. Esto fomenta que el agente explore el espacio de acción y evita que se quede atascado en una política que sea inferior a la política óptima pendiente de ser descubierta.\n",
    "+ **`episodios = 1000`**: Define cuántas veces el agente va a repetir el entrenamiento completo. Mientras más alto sea el número de `episodios`, el agente tendrá más oportunidades para aprender de diferentes situaciones. En este caso yo considero que `1000` es un número suficientemente grande para permitir que el agente explore y aprenda adecuadamente en muchos entornos.\n",
    "\n",
    "Ahora que todas las variables están definidas, comencemos a definir las **funciones** que va a necesitar nuestro algoritmo.\n",
    "\n",
    "Primero vamos a necesitar una variable que se encargue de convertir el **estado* en un **índice lineal**. Recuerda que el estado consiste en *dos dimensiones*, pero el algoritmo va a necesitar ubicarlo en un *índice único* en nuestra nueva **tabla Q**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a287a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estado_a_indice(estado):\n",
    "    return estado[0] * dimensiones[1] + estado[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52249b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estado_a_indice((3, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf908ad0",
   "metadata": {},
   "source": [
    "La segunda función auxiliar se va a encargar de decidir si la siguiente acción va a ser por explotación o por exploración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82052299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elegir_accion(estado):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, num_acciones - 1)\n",
    "    else:\n",
    "        return np.argmax(Q[estado_a_indice(estado)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfeb107f",
   "metadata": {},
   "source": [
    "Y ahora que tenemos una función que **elige** la siguiente acción, vamos a definir una función que se encargue de **aplicar** esa acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bd22e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_accion(estado, accion_idx):\n",
    "    accion = acciones[accion_idx]\n",
    "    nuevo_estado = tuple(np.add(estado, accion) % np.array(dimensiones))\n",
    "    \n",
    "    if nuevo_estado == estado_objetivo:\n",
    "        recompensa = 1\n",
    "    else:\n",
    "        recompensa = -1\n",
    "    \n",
    "    return nuevo_estado, recompensa, nuevo_estado == estado_objetivo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8e1fb27",
   "metadata": {},
   "source": [
    "Antes de avanzar quiero explicar cuál es el sentido de la variable `nuevo_estado`. Vamos a analizar su contenido.\n",
    "\n",
    "+ **`np.add(estado, accion)`**: Esta función de **NumPy** suma el *estado actual* y la *accion* que se va a tomar. Si *estado* es una **tupla** con la posición actual del agente en la cuadrícula (por ejemplo `(x, y)`) y *accion* es una **tupla** que representa el cambio en la posición debido a la acción (por ejemplo `(-1, 0)` para moverse hacia arriba), esta operación da como resultado la nueva posición del agente.  \n",
    "+ **`% np.array(dimensiones)`**: Este operador de **módulo** asegura que si el agente intenta moverse más allá de los límites de la cuadrícula, la posición se \"envuelve\" al otro lado. ¿Cómo es esto? La función `np.array(dimensiones)` convierte las dimensiones de la cuadrícula en un **array de NumPy** para que la operación de módulo se aplique elemento por elemento. Es una forma de aplicar límites \"atravesables\", donde al cruzar un borde de la tabla se aparece por el lado opuesto (como en los juegos clásicos de \"Asteroids\" o el \"Pac-Man\").\n",
    "+ **`tuple()`**: Convierte el resultado de vuelta en una **tupla**, ya que `np.add()` devuelve un **array de NumPy** y las operaciones subsiguientes también producen un array.\n",
    "\n",
    "En resumen, esta línea actualiza la *posición del agente* (basándose en la acción que ha decidido tomar) y se asegura que la nueva posición se mantenga dentro de los límites del entorno.\n",
    "\n",
    "Ten en cuenta que la decisión de que el agente pueda el comportamiento Pac-Maan de salir por la derecha y aparecer por la izquierda, es una decisión que yo he tomado para este ejemplo, pero se podría preferir algo distinto.\n",
    "\n",
    "\n",
    "### Entrenamiento del modelo\n",
    "\n",
    "Y ahora sí, finalmente tenemos todos los recursos que necesitamos (variables y funciones), y podemos comenzar a entrenar nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d23dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodio in range(episodios):\n",
    "    estado = estado_inicial\n",
    "    accion_idx = elegir_accion(estado)\n",
    "    terminado = False\n",
    "    \n",
    "    while not terminado:\n",
    "        nuevo_estado, recompensa, terminado = aplicar_accion(estado, accion_idx)\n",
    "        nueva_accion_idx = elegir_accion(nuevo_estado)\n",
    "        \n",
    "        indice = estado_a_indice(estado)\n",
    "        Q[indice, accion_idx] += alpha * (recompensa + gamma * Q[estado_a_indice(nuevo_estado), nueva_accion_idx] - Q[indice, accion_idx])\n",
    "        \n",
    "        estado, accion_idx = nuevo_estado, nueva_accion_idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ded61b7d",
   "metadata": {},
   "source": [
    "Con esto, nuestro modelo ha realizado 1000 rondas de entrenamiento, por lo que podemos estar seguros de que ha desarrollado todo el aprendizaje necesario para ser capaz de identificar la política más óptima.\n",
    "\n",
    "\n",
    "### Visualizar los resultados\n",
    "\n",
    "La última etapa de nuestro código va a consistir en la implementación de una **visualización**, para que podemos ver cuál es la política que ha descubierto nuestro agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "954d463b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['', '', '', ''],\n",
       "       ['', '', '', ''],\n",
       "       ['', '', '', ''],\n",
       "       ['', '', '', '']], dtype='<U2')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politica_simbolos = np.empty(dimensiones, dtype='<U2')\n",
    "politica_simbolos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b8c7efb",
   "metadata": {},
   "source": [
    "Ahí tenemos la matriz en la que vamos a visualizar los símbolos de los movimientos con las flechas, pero por ahora está vacía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f18af43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['←', '←', '↓', '←'],\n",
       "       ['↑', '↓', '↑', '←'],\n",
       "       ['↑', '↑', '→', '→'],\n",
       "       ['↑', '↓', '↓', '↑']], dtype='<U2')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(dimensiones[0]):\n",
    "    for j in range(dimensiones[1]):\n",
    "        estado = (i, j)\n",
    "        mejor_accion = np.argmax(Q[estado_a_indice(estado)])\n",
    "        politica_simbolos[i, j] = acciones_simbolos[mejor_accion]\n",
    "\n",
    "politica_simbolos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37bd5961",
   "metadata": {},
   "source": [
    "Gracias a esta acción ya tenemos la matriz rellenada con el símbolo que corresponda al mejor movimiento para cada estado. \n",
    "\n",
    "Espero que hayas disfrutado de esta lección. Hemos aprendido nuestro segundo algoritmo de **Aprendizaje por Reforzamiento**, pero el día aún no se termina porque hay más algoritmos que aprender, y te recomiendo hacerlo ya mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da8a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
